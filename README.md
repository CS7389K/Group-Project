# TurtleBot3 Vision-Language Model Perception

Vision-Language Model (VLM) perception system for TurtleBot3 with Moondream2 on Jetson Xavier NX 8GB.

## Quick Links

- [Overleaf (Requires Access)](https://www.overleaf.com/8494251454nmnssbytfkyk#ee7bf5)

## ğŸ¯ Project Overview

This project implements a hybrid perception pipeline for TurtleBot3 mobile manipulation using:
- **YOLO11** for fast object detection (30+ FPS)
- **Moondream2** VLM for physics-aware reasoning (2-3 Hz)
- **Decision fusion** for manipulation planning

The system enables the robot to:
1. **See**: Detect objects in camera feed
2. **Think**: Reason about physical properties (material, weight, fragility, graspability)
3. **Act**: Make informed manipulation decisions (GRASP, PUSH, AVOID, IGNORE)

## ğŸ“¦ Repository Structure

```
moondream2_turtlebot3/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ docker-compose.yml                 # Docker configuration
â”œâ”€â”€ Dockerfile                         # Container setup
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ system_config.yaml            # System configuration
â”œâ”€â”€ scripts/                          # Standalone test scripts
â”‚   â”œâ”€â”€ complete_hybrid_system.py     # Complete hybrid demo
â”‚   â”œâ”€â”€ ros_vision_agent.py           # Legacy ROS2 prototype
â”‚   â”œâ”€â”€ rpi_camera_node.py            # Legacy camera node
â”‚   â””â”€â”€ test_moondream*.py            # Model testing scripts
â””â”€â”€ src/
    â”œâ”€â”€ perception/                    # ğŸ†• ROS2 Foxy Package
    â”‚   â”œâ”€â”€ README.md                  # Package documentation
    â”‚   â”œâ”€â”€ package.xml                # ROS2 package manifest
    â”‚   â”œâ”€â”€ setup.py                   # Python package setup
    â”‚   â”œâ”€â”€ install.sh                 # Quick setup script
    â”‚   â”œâ”€â”€ requirements.txt           # Python dependencies
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â””â”€â”€ perception_params.yaml # Configuration
    â”‚   â”œâ”€â”€ launch/
    â”‚   â”‚   â”œâ”€â”€ vlm_perception.launch.py    # Full system
    â”‚   â”‚   â””â”€â”€ camera_only.launch.py       # Camera test
    â”‚   â””â”€â”€ turtlebot3_vlm_perception/
    â”‚       â”œâ”€â”€ camera_publisher.py    # Camera streaming node
    â”‚       â””â”€â”€ vlm_reasoner.py        # VLM reasoning node
    â”œâ”€â”€ control/                       # (Future) Motion control
    â””â”€â”€ planning/                      # (Future) Task planning
```

## ğŸš€ Quick Start

### For TurtleBot3 Deployment (ROS2 Package)

The main ROS2 package is located in `src/perception/`. This is production-ready code for deployment on TurtleBot3.

**See detailed instructions**: [`src/perception/README.md`](src/perception/README.md)

**Quick install**:
```bash
cd src/perception
chmod +x install.sh
./install.sh
```

**Quick run**:
```bash
# Enable max performance
sudo nvpmodel -m 0
sudo jetson_clocks

# Launch full system
source ~/ros2_ws/install/setup.bash
ros2 launch turtlebot3_vlm_perception vlm_perception.launch.py
```

### For Development/Testing (Standalone Scripts)

The `scripts/` folder contains standalone test scripts for development:

- `complete_hybrid_system.py` - Complete hybrid demo (no ROS2)
- `test_moondream*.py` - Various model testing scripts
- Legacy prototypes (not for production use)

**Run standalone demo**:
```bash
cd scripts
python3 complete_hybrid_system.py
```

## ğŸ¤– Hardware Requirements

- **Robot**: TurtleBot3 (Burger/Waffle/Waffle Pi)
- **Computer**: Nvidia Jetson Xavier NX 8GB
- **Camera**: Raspberry Pi Camera Module v2 or HQ
- **Manipulator**: OpenMANIPULATOR-X (optional)

## ğŸ’» Software Stack

- **OS**: Ubuntu 20.04
- **ROS**: ROS2 Foxy
- **ML Framework**: PyTorch 1.10+ (Jetson-optimized)
- **VLM**: Moondream2 (8-bit quantized)
- **Detection**: YOLO11n (Nano model)

## ğŸ“Š Performance Targets

| Component | Target | Actual (Jetson Xavier NX) |
|-----------|--------|---------------------------|
| Camera Stream | 30 FPS | 30 FPS |
| YOLO11 Detection | 30+ FPS | 30-40 FPS |
| Moondream2 VLM | 2-3 Hz | 2-3 Hz |
| End-to-End System | 2-3 Hz | 2-3 Hz |
| Memory Usage | <6GB RAM | ~5-6GB RAM |
| GPU Usage | <4GB | ~3-4GB |

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TurtleBot3 + Jetson Xavier NX            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Node A:         â”‚  Image  â”‚ Node B:                 â”‚  â”‚
â”‚  â”‚ Camera          â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ VLM Reasoner            â”‚  â”‚
â”‚  â”‚ Publisher       â”‚ 30 FPS  â”‚                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚         â”‚                    â”‚ â”‚ YOLO11 Detection   â”‚ â”‚  â”‚
â”‚         â”‚                    â”‚ â”‚ (30+ FPS)          â”‚ â”‚  â”‚
â”‚    RPi Camera v2             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚    (GStreamer HW Accel)      â”‚            â”‚            â”‚  â”‚
â”‚                               â”‚            v            â”‚  â”‚
â”‚                               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚                               â”‚ â”‚ Moondream2 VLM     â”‚ â”‚  â”‚
â”‚                               â”‚ â”‚ Physics Reasoning  â”‚ â”‚  â”‚
â”‚                               â”‚ â”‚ (2-3 Hz)           â”‚ â”‚  â”‚
â”‚                               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚                               â”‚            â”‚            â”‚  â”‚
â”‚                               â”‚            v            â”‚  â”‚
â”‚                               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚                               â”‚ â”‚ Decision Fusion    â”‚ â”‚  â”‚
â”‚                               â”‚ â”‚ GRASP/PUSH/AVOID   â”‚ â”‚  â”‚
â”‚                               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                            â”‚               â”‚
â”‚                                            v               â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                               â”‚ Terminal Dashboard     â”‚   â”‚
â”‚                               â”‚ (Real-time Display)    â”‚   â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Features

### Vision (YOLO11)
- âœ… Real-time object detection
- âœ… 80+ object classes (COCO dataset)
- âœ… Bounding box localization
- âœ… Confidence scoring

### Reasoning (Moondream2)
- âœ… Material identification (plastic, metal, glass, etc.)
- âœ… Weight estimation (light/medium/heavy)
- âœ… Fragility assessment
- âœ… Size estimation
- âœ… Reachability analysis
- âœ… Graspability evaluation

### Decision Making
- âœ… GRASP: Objects within specs (â‰¤500g, 10-100mm, not fragile)
- âœ… PUSH: Heavy or oversized but movable objects
- âœ… AVOID: Fragile or high-risk objects
- âœ… IGNORE: Out of reach or non-blocking objects
- âœ… STOP: Uncertain situations requiring clarification

## ğŸ“ˆ Future Work

- [ ] Integration with motion control (`src/control/`)
- [ ] Task planning module (`src/planning/`)
- [ ] Multi-object scene understanding
- [ ] Grasp pose estimation
- [ ] Real-world manipulation experiments
- [ ] Performance optimization for TurtleBot3 Burger (Raspberry Pi 4)

## ğŸ› Known Issues

1. **bitsandbytes**: May not install on Jetson (fallback to FP16)
2. **Memory**: First VLM query may take 10-15s (model loading)
3. **Camera**: Requires proper cable seating and permissions

## ğŸ“š Documentation

- **Package README**: `src/perception/README.md` (detailed setup & usage)
- **Configuration**: `src/perception/config/perception_params.yaml`
- **API Reference**: See docstrings in Python files

## ğŸ™ Acknowledgments

- [Moondream2](https://huggingface.co/vikhyatk/moondream2) by Vikhyat Korrapati
- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
- [ROS2](https://docs.ros.org/en/foxy/)
- [Nvidia Jetson](https://developer.nvidia.com/embedded/jetson-xavier-nx)

## ğŸ“„ License

MIT License

## ğŸ‘¥ Team

CS7389K Group Project
